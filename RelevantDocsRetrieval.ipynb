{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "import sys\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from html.parser import HTMLParser\n",
    "from html.entities import name2codepoint\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom HTMLParser class to parse text in SGML format\n",
    "\"\"\"\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    \n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.titleTag = False\n",
    "        self.textTag = False\n",
    "        self.docnoTag = False\n",
    "        self.lasttag = None\n",
    "        self.title = None\n",
    "        self.text = None\n",
    "        self.docno = None\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        #print(\"Start tag:\", tag)\n",
    "        if tag == 'docno':\n",
    "            self.docnoTag = True\n",
    "            self.lasttag = tag\n",
    "                \n",
    "        elif tag == 'title':\n",
    "            self.titleTag = True\n",
    "            self.lasttag = tag\n",
    "            \n",
    "        elif tag == 'text':\n",
    "            self.textTag = True\n",
    "            self.lasttag = tag\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        #print(\"End tag  :\", tag)\n",
    "        if tag == 'docno':\n",
    "            self.docnoTag = False\n",
    "                \n",
    "        elif tag == 'title':\n",
    "            self.titleTag = False\n",
    "            \n",
    "        elif tag == 'text':\n",
    "            self.textTag = False\n",
    "\n",
    "    def handle_data(self, data):\n",
    "\n",
    "        if self.lasttag == 'docno' and self.docnoTag:\n",
    "            self.docno = data\n",
    "            #print('data docno:', data)\n",
    "        elif self.lasttag == 'title' and self.titleTag:\n",
    "            self.title = data\n",
    "            #print('data title:', data)\n",
    "        elif self.lasttag == 'text' and self.textTag:\n",
    "            self.text = data\n",
    "            #print('data text:',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    "    param1: Filename \n",
    "\n",
    "Returns:\n",
    "    File content as text of the file passed as param \n",
    "\"\"\"\n",
    "def getFileContent(fname):\n",
    "    file_content = ''\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "        for word in content:\n",
    "            file_content += word.strip() + \" \"\n",
    "        \n",
    "    return file_content\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: text \n",
    "\n",
    "Returns:\n",
    "    tokens from WordPunctTokenizer\n",
    "\"\"\"\n",
    "def custom_tokenize(text):\n",
    "    tokens = WordPunctTokenizer().tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Returns:\n",
    "    stop words from nltk corpus\n",
    "\"\"\"\n",
    "def getNltkCorpusStopWords():\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    return stop_words\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: word list as words \n",
    "    param2: stop_words\n",
    "\n",
    "Returns:\n",
    "    stemmed words after stemming, digits removal, short and stop words removal \n",
    "\"\"\"\n",
    "def stemmingDigitsShortAndStopWordsRemoval(words, stop_words):\n",
    "    \n",
    "    stemmed_words = list()\n",
    "    ps = PorterStemmer()\n",
    "    for word in words:\n",
    "        if word not in stop_words and word.isalnum() and not word.isdigit():\n",
    "            stemmed_word = ps.stem(word)\n",
    "            #stemmed_word = word\n",
    "            if len(stemmed_word) > 2 and word not in stop_words:\n",
    "                stemmed_words.append(stemmed_word)\n",
    "        \n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: filenames\n",
    "\n",
    "Returns:\n",
    "    A dictionary, key value pair of docs and its contents as text \n",
    "\"\"\"\n",
    "def parse_SGML_text(filenames):\n",
    "    \n",
    "    all_files_content_dict = dict()\n",
    "    html_parser = MyHTMLParser()\n",
    "    \n",
    "    for filename in filenames:\n",
    "        file_content = getFileContent(filename)\n",
    "\n",
    "        html_parser.feed(file_content)\n",
    "        text = html_parser.text\n",
    "        title = html_parser.title\n",
    "        docno = html_parser.docno\n",
    "        all_files_content_dict[docno.strip()] = (title + ' ' + text).lower()\n",
    "        \n",
    "    return all_files_content_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: all_files_content_dict: A dictionary, key value pair of \n",
    "    docs and its contents as text\n",
    "\n",
    "Returns:\n",
    "    A dictionary, key value pair of docs/queries and its contents as list of words\n",
    "\"\"\"\n",
    "def data_preprocessing(all_files_content_dict):\n",
    "    \n",
    "    stop_words = getNltkCorpusStopWords() \n",
    "    for key, value in all_files_content_dict.items():\n",
    "        \n",
    "        doc_id = key\n",
    "        file_text = value\n",
    "        \n",
    "        tokens = custom_tokenize(file_text)\n",
    "        words = stemmingDigitsShortAndStopWordsRemoval(tokens, stop_words)\n",
    "        all_files_content_dict[doc_id] = words\n",
    "    \n",
    "    return all_files_content_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: queries_doc_path\n",
    "\n",
    "Returns:\n",
    "    A dictionary, key value pair of query id and its contents as text\n",
    "\"\"\"\n",
    "def get_queries_content_dict(queries_doc_path):\n",
    "    \n",
    "    queries_content_dict = dict()\n",
    "    with open(queries_doc_path) as fp:  \n",
    "        for cnt, line in enumerate(fp):\n",
    "            queries_content_dict[str(cnt + 1)] = line\n",
    "    fp.close()\n",
    "    return queries_content_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: queries_content_dict: A dictionary, key value pair of \n",
    "    queries and its contents as list of words\n",
    "\n",
    "Returns:\n",
    "    A dictionary, key value pair of query id and its contents as text\n",
    "\"\"\"\n",
    "def calculate_queries_term_freq(queries_content_dict):\n",
    "    \n",
    "    queries_tf_dict = dict()\n",
    "    \n",
    "    for key, value in queries_content_dict.items():\n",
    "        \n",
    "        words = value\n",
    "        query_file_vocab = dict()\n",
    "        max_freq = 0\n",
    "        for word in words:\n",
    "            \n",
    "            if word in query_file_vocab:\n",
    "                query_file_vocab[word] += 1\n",
    "            else:\n",
    "                query_file_vocab[word] = 1\n",
    "                \n",
    "            if query_file_vocab[word] > max_freq:\n",
    "                max_freq = query_file_vocab[word]\n",
    "                \n",
    "        queries_tf_dict[key] = [max_freq, query_file_vocab]\n",
    "        \n",
    "    return queries_tf_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: all_files_content_dict: A dictionary, key value pair of \n",
    "    docs and its contents as list of words\n",
    "\n",
    "Returns:\n",
    "    all_files_vocab, posting list\n",
    "\"\"\"\n",
    "def posting_list_creation(all_files_content_dict):\n",
    "    \n",
    "    doc_max_freq_dict = dict()\n",
    "    all_files_vocab = dict()\n",
    "    \n",
    "    for key, value in all_files_content_dict.items():\n",
    "        \n",
    "        words = value\n",
    "        file_vocab = {}\n",
    "        max_word_freq = 0\n",
    "        for word in words:\n",
    "\n",
    "            if word in file_vocab:\n",
    "                file_vocab[word] += 1\n",
    "\n",
    "                doc_term_freq_dic = all_files_vocab[word][1]\n",
    "                doc_term_freq_dic[key] += 1\n",
    "                all_files_vocab[word] = [all_files_vocab[word][0], doc_term_freq_dic]\n",
    "            else:\n",
    "                file_vocab[word] = 1\n",
    "                if word in all_files_vocab:\n",
    "                    doc_term_freq_dict = all_files_vocab[word][1]\n",
    "                    doc_term_freq_dict[key] = 1\n",
    "                    all_files_vocab[word] = [(all_files_vocab[word][0]) + 1, doc_term_freq_dict]\n",
    "                else:\n",
    "                    all_files_vocab[word] = [1, {key: 1}]\n",
    "                    \n",
    "            if file_vocab[word] > max_word_freq:\n",
    "                max_word_freq = file_vocab[word]\n",
    "                    \n",
    "        doc_max_freq_dict[key] = max_word_freq\n",
    "        \n",
    "    return all_files_vocab, doc_max_freq_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: all_files_vocab, posting list\n",
    "    param2: doc_max_freq_dict\n",
    "    param3: N, total number of dcos\n",
    "\n",
    "Returns:\n",
    "    documents length dictionay\n",
    "\"\"\"\n",
    "def calculate_docs_length(all_files_vocab, doc_max_freq_dict, N):\n",
    "\n",
    "    docs_length_dict = dict()\n",
    "    \n",
    "    for key, value in all_files_vocab.items():\n",
    "        \n",
    "        df = all_files_vocab[key][0]\n",
    "        \n",
    "        for k, v in all_files_vocab[key][1].items():\n",
    "            \n",
    "            doc_id = k\n",
    "            tf = v\n",
    "            #max_freq = doc_max_freq_dict[doc_id]\n",
    "            idf = math.log2(N/df)\n",
    "            \n",
    "            if doc_id in docs_length_dict:\n",
    "                docs_length_dict[doc_id] += math.pow(tf * idf, 2)\n",
    "            else:\n",
    "                docs_length_dict[doc_id] = math.pow(tf * idf, 2)\n",
    "                \n",
    "    return docs_length_dict\n",
    "\n",
    "\n",
    "def get_value_based_sorted_dict(val_dict):\n",
    "    from collections import OrderedDict\n",
    "    sorted_dict = OrderedDict(sorted(val_dict.items(), key=lambda x: x[1],  reverse=True))\n",
    "    return sorted_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: relevance doc path\n",
    "    param3: N, total number of dcos\n",
    "\n",
    "Returns:\n",
    "    A dictionary of relevant docs for each query\n",
    "\"\"\"\n",
    "def getRelevantQuerydocs(relevance_doc_path):\n",
    "    \n",
    "    queries_relevant_docs_dict = dict()\n",
    "    with open(relevance_doc_path) as fp:  \n",
    "        for line in fp:\n",
    "            line_text_list = line.strip().split()\n",
    "            q_id = line_text_list[0]\n",
    "            rel_doc_no = line_text_list[1]\n",
    "            if q_id in queries_relevant_docs_dict:\n",
    "                queries_relevant_docs_dict[q_id].add(rel_doc_no)\n",
    "            else:\n",
    "                queries_relevant_docs_dict[q_id] = {rel_doc_no}\n",
    "                \n",
    "    return queries_relevant_docs_dict\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: docs_length_dict\n",
    "    param2: all_files_vocab\n",
    "    param3: doc_max_freq_dict\n",
    "    param4: queries_tf_dict\n",
    "    param5: N, number of all docs\n",
    "\n",
    "Returns:\n",
    "    A dictionary, key value pair of queries and docs(sorted based on cosine similarity)\n",
    "\"\"\"\n",
    "def calculate_cosine_similarities(docs_length_dict, all_files_vocab, doc_max_freq_dict, queries_tf_dict, N):\n",
    "    \n",
    "    all_queries_cosine_similarities = dict()\n",
    "    \n",
    "    for key in queries_tf_dict:\n",
    "        q_id = key\n",
    "        q_cosine_scores_dict = calculate_query_cosine_similarities(q_id, docs_length_dict, all_files_vocab, doc_max_freq_dict, queries_tf_dict, N)\n",
    "        q_Sorted_cosine_scores_dict = get_value_based_sorted_dict(q_cosine_scores_dict)\n",
    "        \n",
    "        all_queries_cosine_similarities[q_id] = [key for key in q_Sorted_cosine_scores_dict]\n",
    "        \n",
    "    return all_queries_cosine_similarities\n",
    "\n",
    "\"\"\"\n",
    "Args:\n",
    "    param1: q_id\n",
    "    param2: docs_length_dict\n",
    "    param3: all_files_vocab\n",
    "    param4: doc_max_freq_dict\n",
    "    param5: queries_tf_dict\n",
    "    param6: N, number of all docs\n",
    "\n",
    "Returns:\n",
    "    A dictionary of cosine_similarities of docs for a query\n",
    "\"\"\"\n",
    "def calculate_query_cosine_similarities(q_id, docs_length_dict, all_files_vocab, doc_max_freq_dict, queries_tf_dict, N):\n",
    "    \n",
    "    #q_max_freq = queries_tf_dict[q_id][0]\n",
    "    q_tf_dict = queries_tf_dict[q_id][1]\n",
    "\n",
    "    q_cosine_scores_dict = dict()\n",
    "    q_term_weight_sq_sum = 0\n",
    "    \n",
    "    for k, v in q_tf_dict.items():\n",
    "\n",
    "        word = k\n",
    "        q_term_freq = v\n",
    "        \n",
    "        if word in all_files_vocab:\n",
    "        \n",
    "            df = all_files_vocab[word][0]\n",
    "            idf = math.log2(N/df)\n",
    "            \n",
    "            q_term_weight = q_term_freq * idf\n",
    "            q_term_weight_sq_sum += math.pow(q_term_weight, 2)\n",
    "\n",
    "            for key, value in all_files_vocab[word][1].items():\n",
    "                \n",
    "                doc_id = key\n",
    "                doc_tf = value\n",
    "\n",
    "                #doc_max_freq = doc_max_freq_dict[doc_id]\n",
    "                if doc_id in q_cosine_scores_dict:\n",
    "                    q_cosine_scores_dict[doc_id] += q_term_weight * (doc_tf * idf)\n",
    "                else:\n",
    "                    q_cosine_scores_dict[doc_id] = q_term_weight * (doc_tf * idf)\n",
    "                \n",
    "    for ke, val in q_cosine_scores_dict.items():\n",
    "        q_cosine_scores_dict[ke] = val / math.sqrt(docs_length_dict[ke] * q_term_weight_sq_sum)\n",
    "                \n",
    "    return q_cosine_scores_dict\n",
    "\n",
    "\"\"\"\n",
    "Calculate and prints average precision, recall of all queries for topN ranked documents\n",
    "Args:\n",
    "    param1: all_queries_cosine_similarities\n",
    "    param2: queries_relevant_docs_dict\n",
    "    param3: topN\n",
    "\"\"\"\n",
    "def calculate_precision_recall(all_queries_cosine_similarities, queries_relevant_docs_dict, topN):\n",
    "    \n",
    "    q_N = len(all_queries_cosine_similarities)\n",
    "    avg_precision = 0\n",
    "    avg_recall = 0\n",
    "    precision_recall_dict = dict()\n",
    "    for key in all_queries_cosine_similarities:\n",
    "        \n",
    "        q_id = key\n",
    "        retrieved_q_docs = all_queries_cosine_similarities[q_id]\n",
    "        retrieved_q_docs = retrieved_q_docs[:topN]\n",
    "        \n",
    "        relevant_q_docs = queries_relevant_docs_dict[q_id]\n",
    "        TP = 0\n",
    "        for doc in retrieved_q_docs:\n",
    "            if doc in relevant_q_docs:\n",
    "                TP += 1\n",
    "        recall = TP/len(relevant_q_docs)\n",
    "        precision = TP/topN\n",
    "        precision_recall_dict[q_id] = [precision, recall]\n",
    "\n",
    "        avg_precision += precision\n",
    "        avg_recall += recall\n",
    "        \n",
    "    avg_precision = avg_precision/q_N\n",
    "    avg_recall = avg_recall /q_N\n",
    "    \n",
    "    print(' ')\n",
    "    print(\"Precision Recall for top \",topN,\" retrieved documents in rank list: \")\n",
    "    for k, val in precision_recall_dict.items():\n",
    "        print(\"Query: \",k,\"     Pr: \",val[0],\"      Re: \",val[1])\n",
    "    \n",
    "    print('Average Precision for ',topN,' docs: ',avg_precision)\n",
    "    print('Average Recall for ',topN,' docs: ',avg_recall)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Precision Recall for top  10  retrieved documents in rank list: \n",
      "Query:  1      Pr:  0.0       Re:  0.0\n",
      "Query:  2      Pr:  0.3       Re:  0.2\n",
      "Query:  3      Pr:  0.3       Re:  0.2\n",
      "Query:  4      Pr:  0.1       Re:  0.05555555555555555\n",
      "Query:  5      Pr:  0.1       Re:  0.05263157894736842\n",
      "Query:  6      Pr:  0.4       Re:  0.2222222222222222\n",
      "Query:  7      Pr:  0.5       Re:  0.5555555555555556\n",
      "Query:  8      Pr:  0.2       Re:  0.5\n",
      "Query:  9      Pr:  0.0       Re:  0.0\n",
      "Query:  10      Pr:  0.2       Re:  0.08333333333333333\n",
      "Average Precision for  10  docs:  0.21000000000000002\n",
      "Average Recall for  10  docs:  0.18692982456140353\n",
      " \n",
      "Precision Recall for top  50  retrieved documents in rank list: \n",
      "Query:  1      Pr:  0.0       Re:  0.0\n",
      "Query:  2      Pr:  0.12       Re:  0.4\n",
      "Query:  3      Pr:  0.18       Re:  0.6\n",
      "Query:  4      Pr:  0.04       Re:  0.1111111111111111\n",
      "Query:  5      Pr:  0.12       Re:  0.3157894736842105\n",
      "Query:  6      Pr:  0.14       Re:  0.3888888888888889\n",
      "Query:  7      Pr:  0.16       Re:  0.8888888888888888\n",
      "Query:  8      Pr:  0.06       Re:  0.75\n",
      "Query:  9      Pr:  0.12       Re:  0.75\n",
      "Query:  10      Pr:  0.06       Re:  0.125\n",
      "Average Precision for  50  docs:  0.1\n",
      "Average Recall for  50  docs:  0.43296783625731\n",
      " \n",
      "Precision Recall for top  100  retrieved documents in rank list: \n",
      "Query:  1      Pr:  0.0       Re:  0.0\n",
      "Query:  2      Pr:  0.12       Re:  0.8\n",
      "Query:  3      Pr:  0.12       Re:  0.8\n",
      "Query:  4      Pr:  0.07       Re:  0.3888888888888889\n",
      "Query:  5      Pr:  0.11       Re:  0.5789473684210527\n",
      "Query:  6      Pr:  0.09       Re:  0.5\n",
      "Query:  7      Pr:  0.09       Re:  1.0\n",
      "Query:  8      Pr:  0.03       Re:  0.75\n",
      "Query:  9      Pr:  0.07       Re:  0.875\n",
      "Query:  10      Pr:  0.04       Re:  0.16666666666666666\n",
      "Average Precision for  100  docs:  0.074\n",
      "Average Recall for  100  docs:  0.5859502923976609\n",
      " \n",
      "Precision Recall for top  500  retrieved documents in rank list: \n",
      "Query:  1      Pr:  0.002       Re:  1.0\n",
      "Query:  2      Pr:  0.03       Re:  1.0\n",
      "Query:  3      Pr:  0.03       Re:  1.0\n",
      "Query:  4      Pr:  0.03       Re:  0.8333333333333334\n",
      "Query:  5      Pr:  0.038       Re:  1.0\n",
      "Query:  6      Pr:  0.036       Re:  1.0\n",
      "Query:  7      Pr:  0.018       Re:  1.0\n",
      "Query:  8      Pr:  0.008       Re:  1.0\n",
      "Query:  9      Pr:  0.016       Re:  1.0\n",
      "Query:  10      Pr:  0.032       Re:  0.6666666666666666\n",
      "Average Precision for  500  docs:  0.024\n",
      "Average Recall for  500  docs:  0.95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Retrieves relevant docs from a collection for given queries and\n",
    "calculates average precision and recall based on given relevance text\n",
    "Args:\n",
    "    param1: data_path, path of data collection\n",
    "    param2: queries_doc_path, path of queries text file\n",
    "    param3: path of relevance text file\n",
    "\"\"\"\n",
    "def relevant_docs_retrieval(data_path, queries_doc_path, relevance_doc_path):\n",
    "    \n",
    "    filenames = glob.glob(data_path)\n",
    "    N = len(filenames)\n",
    "    \n",
    "    all_files_content_dict = parse_SGML_text(filenames)\n",
    "    queries_content_dict = get_queries_content_dict(queries_doc_path)\n",
    "    \n",
    "    all_files_content_dict = data_preprocessing(all_files_content_dict)\n",
    "    queries_content_dict = data_preprocessing(queries_content_dict)\n",
    "    \n",
    "    all_files_vocab, doc_max_freq_dict = posting_list_creation(all_files_content_dict)\n",
    "    docs_length_dict = calculate_docs_length(all_files_vocab, doc_max_freq_dict, N)\n",
    " \n",
    "    queries_tf_dict = calculate_queries_term_freq(queries_content_dict)\n",
    "    \n",
    "    all_queries_cosine_similarities = calculate_cosine_similarities(docs_length_dict, all_files_vocab, doc_max_freq_dict, queries_tf_dict, N)\n",
    "    queries_relevant_docs_dict = getRelevantQuerydocs(relevance_doc_path)\n",
    "\n",
    "    calculate_precision_recall(all_queries_cosine_similarities, queries_relevant_docs_dict, 10)\n",
    "    calculate_precision_recall(all_queries_cosine_similarities, queries_relevant_docs_dict, 50)\n",
    "    calculate_precision_recall(all_queries_cosine_similarities, queries_relevant_docs_dict, 100)\n",
    "    calculate_precision_recall(all_queries_cosine_similarities, queries_relevant_docs_dict, 500)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    data_path = 'cranfield.tar/cranfieldDocs/*'\n",
    "    queries_doc_path = 'queries.txt'\n",
    "    relevance_doc_path = 'relevance.txt'\n",
    "#     data_path = sys.argv[1]\n",
    "#     queries_doc_path = sys.argv[2]\n",
    "#     relevance_doc_path = sys.argv[3]\n",
    "\n",
    "    relevant_docs_retrieval(data_path, queries_doc_path, relevance_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
